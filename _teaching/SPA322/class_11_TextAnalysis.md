---
layout: materials
title: SPA 322
permalink: /teaching/SPA322/:title.html
collection: teaching
---
Miércoles 3 de Octubre de 2017

## 3. Las dimensión digital de los textos

### 1.	IntroducciónEn humanidades digitales, el concepto de “texto”, como hemos visto, puede ser entendido como una idea abstracta pero también como datos reales; como tal, el texto representa el núcleo de cualquier investigación lingüística y literaria. El análisis textual puede llevarse a cabo a través de muchos y variados mecanismos. Ahora bien, desde los años cincuenta existen una serie de procedimientos que establecen los métodos para el análisis electrónico de textos. El objetivo consiste en manipular de manera informática los textos y extraer informaciones que de una manera manual sería demasiado laboriosa, como por ejemplo, listas alfabéticas, concordancias o frecuencias de palabras.El análisis electrónico de textos puede ser muy útil, entre otras cosas para: 

* encontrar patrones gramaticales
* frases recurrentes
* frecuencia de ciertas palabras en un corpus o en un autor
* ejemplos de un concepto
Para llevar a cabo cualquier análisis de textos, necesitamos un corpus, es decir, una serie de datos textuales. Las etapas son las siguientes: 

* Construcción de un corpus: importancia del nombre de los ficheros, de la codificación en UTF-8, del formato en .txt, 
* Limpieza (Clean-up)

Actualmente se habla de **“data mining”** o minería de datos que, en un sentido amplio, se refiere al proceso de análisis de un conjunto de datos para extraer información de ellos. Entendido así podríamos referirnos a una simple búsqueda por palabra en un documento o en Google. En la disciplina de las ciencias sociales, la minería de datos cuantitativos o informaciones estadísticas ha sido una práctica muy habitual, pero no tanto en humanidades. En el campo de las HD, minería de datos implica la extracción de información a partir de un corpus textual con o sin metadatos, de manera que puedan responderse cuestiones relativas a esos textos. La información extraída normalmente suele ser cuantitativa, en el sentido que los resultados se obtienen bajo forma de números. Aún así, un papel decisivo juega la interpretación de estos resultados basados en cuestiones puramente literarias o lingüísticas. Las herramientas utilizadas para el análisis de textos no son muchas. **En general se trata de programas que cuentan el número de palabras de un corpus, palabras clave, frecuencias, etc.** En esta unidad veremos superficialmente tres programas: [Voyant tools](https://voyant-tools.org/), [AntConc](http://www.laurenceanthony.net/software/antconc/), y [RStudio](https://www.rstudio.com/). Antes de nada, conviene aclarar una cuestión de no poca importancia. Cualquier análisis electrónico de textos se basa un **corpus**, ya sea grande o pequeño, y el primer paso consiste justamente en su creación. Existen una serie de pautas que deben seguirse para que podamos sacar el máximo partido de nuestro texto, como por ejemplo que esté en un formato estándar, a ser posible en texto plano. ### 2. La creación de un corpus#### 2.1. Consideraciones generales Existe una gran diferencia si se trata de un **corpus actual** o de un **corpus de textos antiguos**. El primero, tiene un carácter abierto (pues puede ser todavía enriquecido), puede ser más heterogéneo y plantear más problemas justamente por estar relacionado con la sociedad contemporánea (cuestiones culturales, posibilidad de verificación, etc.). En cambio, un corpus histórico es cerrado, más homogéneo, y plantea problemas relacionados con el hecho de tener sólo textos escritos, sin posibilidad de verificación cultural, y sin referencias concretas que han ya desaparecido; en definitiva, los corpus de textos antiguos son más opacos. En cualquier caso, es preciso conocer los textos antes de analizarlos y partir de algunas intuiciones y objetivos. Al crear un corpus, debemos proceder de una manera lógica, especialmente en la selección de los textos en función de nuestros objetivos de investigación. Los resultados obtenidos a partir del corpus dependen de la naturaleza del mismo. Para los corpus históricos, además, se debe reflexionar sobre las relaciones entre los textos y la sociedad que los produjo. La dimensión es otro elemento crucial. Sean cuales sean sus dimensiones, un corpus siempre da una visión parcial sobre la sociedad, por eso se debe estar bien documentado. La adopción de las herramientas informáticas es un elemento clave, pues son las que ayudan a la percepción del sentido de las palabras y de los textos del corpus. Aunque su utilidad es evidente, se trata siempre de métodos que no reemplazan los métodos tradicionales, sino que los complementan. Las operaciones de estas herramientas no tienen nada de automático, y deben comprenderse desde el interior, debemos ser capaces de modificar por ejemplo los algoritmos con los que se ejecutan, y si es preciso inventar de nuevos. Por ello, es necesario trabajar con programas libres, para poder modificarlos. Antes de iniciar la creación de un corpus debemos definir una **estrategia**. En primer lugar conviene recuperar y organizar el corpus en función del tiempo a disposición, teniendo una idea precisa del tiempo del que disponemos. En función de este, se deben analizar las posibilidades prácticas, y evaluar el tiempo que se necesitará.  En muchas ocasiones, especialmente para trabajos de investigación, más vale un corpus pequeño, pero inteligentemente analizado, que un corpus enorme que podremos analizar solo superficialmente. En cuanto a las **dimensiones** podemos guiarnos con las siguientes cifras: un corpus pequeño no comprende más de 5 millones de tokens; mientras que un corpus mediano contiene entre cinco y quinientos millones de tokens; más allá de estas cifras se trabaja con grandes corpora. Al crear un corpus nos encontramos con el problema del **copyright**. En el caso de los textos antiguos se encuentran en el dominio público, y por tanto nadie dispone de un derecho comercial. La [Convención de Berna](http://www.wipo.int/treaties/es/ip/berne/summary_berne.html) es el texto fundamental sobre la propiedad intelectual. En principio, está previsto que la propiedad intelectual de un autor se pierda al cabo de 50 años de su muerte.  Aún así, en cada país se aplica de manera diferente con lo cual es arriesgado generalizar (véase para cada país, el [Código de la propiedad intelectual](http://www.wipo.int/portal/es/)). En los casos por ejemplo de ediciones, el editor de la obra tiene la propiedad intelectual de sus comentarios y notas, pero no del texto del autor. En el caso de Google, por ejemplo, se distribuyen obras que ya forman parte del dominio público. #### 2.2. Etapas para la construcción de corpus textuales En el mejor de los casos podemos ya disponer de corpus informatizados, pero no siempre es así. A falta de una copia informática del texto, no hay otra alternativa que escanear los textos y pasarlos a través de un programa OCR (Optical Character Recognition). #### Etapa 1. Recuperación de los textos-	Escaneo y OCR. Si no disponemos de una copia digital, deberemos elegir la edición en papel que nos parezca más clara y de más fácil reconocimiento por parte del programa OCR. A veces cuando se realiza el escaneo se deben realizar pruebas para que el escáner actúe de una forma óptima. Casi todos los [programas son de pago](https://computers.tutsplus.com/tutorials/5-ways-to-ocr-documents-on-your-mac--mac-49683), pero hay algunos que son libres y gratuitos: 
-	[tesseract](https://github.com/tesseract-ocr/tesseract): software desarrollado por Google, ejecutable en línea de comando.-	[gImageReader](https://github.com/manisandro/gImageReader): se trabaja con interfaz gráfica, pero solo está disponible para Windows, Linux; permite la ejecución de una fase de aprendizaje por parte del programa-	[Gamera](http://gamera.informatik.hsnr.de/addons/ocr4gamera/): en el caso de las ediciones antiguas, se necesita un programa que funcione únicamente en modo imagen, es decir, un aprendizaje completo de todos los caracteres que el programa puede reconocer en su trabajo. - **Formatos en soporte digital**. Puede ser que tengamos copias en formato PDF, en cuyo caso nos serviremos de los mismos procedimientos OCR si se trata de imágenes. Los PDF más modernos suelen ser en realidad ficheros de textos transformados, de manera que una reconversión puede realizarse incluso en línea de comando, con la función `pdftotext` por ejemplo. 

*EJERCICIO PRUEBA*En algunas ocasiones, los textos están en versión digital pero solo están disponible en páginas HTML. En estos casos, podemos copiar manualmente el texto, de manera completa si aparece el texto completo corrido, o bien reconstruirlo por fragmentos. Existen algunos programas libres que recuperan todas las páginas HTML de un sitio web, como el [GNU WGet](https://www.gnu.org/software/wget/). También en este caso los ficheros deben limpiarse de caracteres y marcas inútiles. *EJERCICIO PRUEBA*
En aquellos casos en que dispongamos ya de una copia en .doc, deberemos simplemente transformar los ficheros en un formato de texto plano, también disponible en línea de comando. 

*EJERCICIO PRUEBA*#### Etapa 2. Corrección de los resultados.Sea cual sea el escáner utilizado, los resultados no son nunca perfectos por lo que siempre es necesario acudir a una etapa de corrección manual de los textos. En primer lugar, el corrector ortográfico puede ayudarnos considerablemente. Lo ideal es contar con un programa que contenga un diccionario con todas las formas aceptadas. Algunos de los software utilizados contienen ya diccionarios, de no ser así pueden utilizarse aplicaciones como [Hunspell](http://hunspell.github.io/), que funciona bien y rápido en un segundo plano en múltiples programas de edición como [Geany](https://www.geany.org/) o [Gedit](https://wiki.gnome.org/Apps/Gedit). Para las lenguas modernas, disponemos de numerosos diccionarios, pero no para las lenguas antiguas que, además, plantean múltiples problemas, como las variaciones gráficas o hapax; la única solución consiste en realizar un diccionario ad hoc o modificar alguno ya existente. #### Etapa 3. Estructuración del corpus El primer paso en la estructuración del corpus es la nomenclatura utilizada para nombrar los ficheros. Para ello, es importante respetar algunas normas, como por ejemplo, utilizar solo minúsculas y números, no usar ningún carácter acentuado, diacrítico, y nunca espacios. Los ficheros se ordenarán de forma alfabética, así que esta es una cuestión que debe ser tomada en cuenta para facilitar su localización. Por ejemplo, si lo que nos interesa es su orden cronológico, mejor utilizar las fechas en el nombre. Conviene también, en fin, establecer algún tipo de codificación o abreviatura lógica y sistemática, por ejemplo: autor, título, categoría… eso permitirá ganar tiempo. La nomenclatura debe ser simple y comprensible a primera vista. Conviene también utilizar siempre una misma codificación de caracteres. En informática los caracteres alfanuméricos son codificados en con un código binario, es decir, que cada letra corresponde a una serie de números binarios o combinación de 0 y 1. Existen diferentes tipos, de entre los cuales los más frecuentes son: ASCII, Windows-1252, ISO 8859-15 (Latin-15), y UTF-8. El único universal y más eficaz es el UTF-8 (Universal Character Set Transformation Format en 8 bits). Todos los programas lo reconocen, y algunos incluso lo exigen, por ello debemos siempre codificar en UTF-8, y comprobar continuamente que estamos recuperando textos con esta codificación; de no ser así, debemos cambiarla, por ejemplo utilizando el comando iconv en el terminal. Así pues, puede darse el caso que dispongamos al principio de una variedad de formatos (texto plano, html, odt, doc, rtg, pdf), en cuyo caso conviene convertirlo todo en texto plano para poder manipular los textos con un editor como TextWrangler, Komodo, Geany, Gedit, etc. A continuación, debemos controlar que los ficheros de nuestro corpus estén bien construidos y no contengan elementos inútiles, códigos raros, etiquetas, etc. Si tenemos pocos documentos, podemos hacerlo de manera manual; en caso contrario, una limpieza a través de un script sería más seguro y homogéneo, pero para ello se necesitan algunos conocimientos de programación. Además, será necesario atribuir una serie de metadatos para cada uno de los ficheros. Normalmente cualquier base de datos textuales contiene informaciones bibliográficas sobre los textos, cuyas categorías suelen ser: autor, título, y fecha. Pero un corpus bien estructurado debería llevar otras informaciones como por ejemplo: género, zona geográfica, prosa/verso, etc. Estas informaciones es mejor empezarlas a recoger desde el principio de nuestro trabajo. Normalmente los metadatos suelen ir en un fichero (csv) separado, con el nombre del fichero y las diferentes informaciones. Otro método de estructuración es el marcado en TEI, en cuyo caso los metadatos van directamente al interno del fichero. Incluso si tenemos muchos ficheros (más de 500), es útil disponer de una pequeña base de datos separada, para poder interrogar nuestro corpus a partir de los metadatos. El formato SQLite es simple de utilizar, así como el SQLitestudio.#### Etapa 4. PretratamientoLa mayoría de los programas se encargan de una manera (semi-)automática de ejecutar estas tareas. En línea general el pretratameinto consisten en las siguientes fases: 
-	[Tokenización](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html): separación del texto en unidades elementales, que no siempre corresponde a la separación por palabras. -	[Lematización](): cada token es enriquecida con su POS (Part of Speech) o categoría gramatical y su lema. 		Ejemplo: <word type=”verb” lemma=“casa”>casa</word> Algunos links útiles: <http://liceu.uab.cat/~joaquim/language_technology/NLP/PLN_analisis.html> , <https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html>**EJERCICIO TOKENIZACIÓN*
#### Etapa 5. Inserción en una base de datosLos ficheros limpios, con los metadatos, y pretratados pueden en fin ser incluidos en una base de datos propiamente dicha. La finalidad es poder consultarlos y poder recuperar toda la información codificada. Existe una gran variedad de motores de indexación, como por ejemplo: 
-	MySQL, Postgre-SQL (motores SQL)-	Open CWB (Open Corpus Workbench): puede utilizarse en el terminal, o a través interfaces gráficas-	TXM-	CQP-Wen-	Uso de bases CWB directamente a partir de programas de estadística como R. Construir una base de datos de textos es una operación larga. La fabricación de un corpus estructurado necesita mecánica y reflexión. Hay una serie de reglas que deben respetarse y existen todavía una gran cantidad de problemas no resueltos. #### 3. Algunas herramientas La mayoría de las herramientas para el análisis electrónico de textos requieren que los ficheros estén en formato de texto plano. Las funciones más frecuentes que podemos encontrarnos son: -	Lista de frecuencias: cuáles son las palabras más utilizadas. Esta función puede tener fines pedagógicos para enseñar el vocabulario más utilizado por ejemplo. -	Concordancias: recuperación de ejemplos; KWIC; ej. facilitar el análisis de conceptos. -	Frecuencias totales, frecuencias relativas, distribución de frecuencias léxicas. -	NGrams Conviene recordar algunos conceptos generales como: token, categoría (gramatical), tipo (palabra), segmento o ngram.El análisis electrónico de textos se considera un estudio cualitativo, pero la realidad es que se basa única y exclusivamente en algoritmos basados en métodos cuantitativos, así como procedimientos de búsqueda para identificar los elementos y las características de los textos. Esto, en realidad, podría considerarse una contradicción. 

#### Otras herramientas útiles: - Textalyser <http://textalyser.net>Herramienta online que ofrece informaciones estadísticas sobre tu texto. Calcula frecuencias y contextos, analiza grupos de palabras, densidad de palabras clase. Puede analizarse un texto propio, e incluso links de páginas web. - Visual Text <http://www.textanalysis.com/index.html>. Este es uno de los pocos programas dedicados exclusivamente al análisis de textos. - Manyeyes (IBM) <http://www-958.ibm.com> Útil para crear visualizaciones con datos ya preparados. - Wordsmith <http://wordsmith.org/> - Google Ngram Viewer: <https://books.google.com/ngrams>- WortSchatz: <http://corpora.informatik.uni-leipzig.de/>- TAPOR: <http://www.tapor.ca/>- Digging into Data Challenge: <http://diggingintodata.org/> Convocatoria de ayudas a la investigación con temas relacionados con la minería de textos y el análisis electrónico de textos. #### Bibliografía: 
- Ide, Nancy. “Preparation and Analysis of Linguistic Corpora.” *A Companion to Digital Humanities*, Blackwell 2004. - Burrows, John. “Textual Analysis.” *A Companion to Digital Humanities*, Blackwell 2004. - Wynne, M (editor). *Developing Linguistic Corpora: a Guide to Good Practice*. Oxford: Oxbow Books, 2005. Disponible en: <http://www.ahds.ac.uk/creating/guides/linguistic-corpora/> [Acceso 10 Nov. 2015].- Megan R. Brett, “Topic Modeling: A Basic Introduction.” *Journal of Digital Humanities* 2.1 (Winter) 2012. - Miriam Posner, “Very basic strategies for interpreting results from the Topic Modeling Tool.” *Miriam Posner’s Blog*, October 28, 2012.